{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bd257c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.15.1-cp310-cp310-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchdata==0.6.0\n",
      "  Downloading torchdata-0.6.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/software/anaconda3/lib/python3.10/site-packages (from torchtext) (4.64.1)\n",
      "Collecting torch==2.0.0\n",
      "  Using cached torch-2.0.0-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Requirement already satisfied: requests in /Users/software/anaconda3/lib/python3.10/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/software/anaconda3/lib/python3.10/site-packages (from torchtext) (1.24.2)\n",
      "Requirement already satisfied: jinja2 in /Users/software/anaconda3/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/software/anaconda3/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/software/anaconda3/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/software/anaconda3/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (3.9.0)\n",
      "Requirement already satisfied: networkx in /Users/software/anaconda3/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (2.8.4)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/software/anaconda3/lib/python3.10/site-packages (from torchdata==0.6.0->torchtext) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/software/anaconda3/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/software/anaconda3/lib/python3.10/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/software/anaconda3/lib/python3.10/site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/software/anaconda3/lib/python3.10/site-packages (from jinja2->torch==2.0.0->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/software/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch==2.0.0->torchtext) (1.2.1)\n",
      "Installing collected packages: torch, torchdata, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0.dev20230408\n",
      "    Uninstalling torch-2.1.0.dev20230408:\n",
      "      Successfully uninstalled torch-2.1.0.dev20230408\n",
      "Successfully installed torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef909fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a4157b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv', nrows=80000).fillna('')\n",
    "df_test = pd.read_csv('dataset/test.csv').fillna('')\n",
    "# df_sample = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa296f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_df = pd.read_csv(\"entities_from_all_text_40000_just_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d63e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_df\n",
    "#ent_df=ent_df.drop(\"TITLE\",axis = \"columns\")\n",
    "#ent_df=ent_df.drop(\"Unnamed: 0\",axis = \"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e1c0b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e60438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['PRODUCT_LENGTH'].astype(int)\n",
    "\n",
    "#X = ent_df\n",
    "X = pd.DataFrame()\n",
    "X['PRODUCT_TYPE_ID'] = df_train['PRODUCT_TYPE_ID']\n",
    "X['TEXT'] =  df_train['TITLE'] + ' ' + df_train['DESCRIPTION'] + ' ' + df_train['BULLET_POINTS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d60da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9066090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f00615ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_data(x):\n",
    "    return [record[:-2].astype(float) for record in x]\n",
    "\n",
    "def get_text_data(x):\n",
    "    return [record[-1] for record in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d92e2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 20 candidates, totalling 140 fits\n"
     ]
    }
   ],
   "source": [
    "transfomer_numeric = FunctionTransformer(get_numeric_data)\n",
    "transformer_text = FunctionTransformer(get_text_data)\n",
    "\n",
    "# Create a pipeline to concatenate Tfidf Vector and Numeric data\n",
    "# Use RandomForestClassifier as an example\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', transfomer_numeric)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', transformer_text),\n",
    "                ('vec', TfidfVectorizer(analyzer='word'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Grid Search Parameters for RandomForest\n",
    "param_grid = {'clf__n_estimators': np.linspace(1, 100, 10, dtype=int),\n",
    "              'clf__min_samples_split': [3, 10],\n",
    "              'clf__min_samples_leaf': [3],\n",
    "              'clf__max_features': [7],\n",
    "              'clf__max_depth': [None],\n",
    "              'clf__criterion': ['gini'],\n",
    "              'clf__bootstrap': [False]}\n",
    "\n",
    "# Training config\n",
    "kfold = KFold(n_splits=7)# StratifiedKFold(n_splits=7)\n",
    "scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
    "refit = 'F1'\n",
    "\n",
    "X_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "# Perform GridSearch\n",
    "rf_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold, scoring=scoring, \n",
    "                         refit=refit, n_jobs=-1, return_train_score=True, verbose=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_best = rf_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e9d216c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604373</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1729783</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871949</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1107571</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>624253</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734731</th>\n",
       "      <td>921419</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734732</th>\n",
       "      <td>2456362</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734733</th>\n",
       "      <td>841529</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734734</th>\n",
       "      <td>1190194</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734735</th>\n",
       "      <td>1040810</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734736 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRODUCT_ID  PRODUCT_LENGTH\n",
       "0           604373             600\n",
       "1          1729783             600\n",
       "2          1871949             600\n",
       "3          1107571             600\n",
       "4           624253             600\n",
       "...            ...             ...\n",
       "734731      921419             600\n",
       "734732     2456362             600\n",
       "734733      841529             600\n",
       "734734     1190194             600\n",
       "734735     1040810             600\n",
       "\n",
       "[734736 rows x 2 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.DataFrame()\n",
    "X_test['PRODUCT_TYPE_ID'] = df_test['PRODUCT_TYPE_ID']\n",
    "X_test['TEXT'] =  df_test['TITLE'] + ' ' + df_test['DESCRIPTION'] + ' ' + df_test['BULLET_POINTS']\n",
    "\n",
    "y_predicted = []\n",
    "\n",
    "# batches of 10,000:\n",
    "test_batch_size = 10000\n",
    "for i in range(0, len(X_test) + test_batch_size, test_batch_size):\n",
    "    try:\n",
    "        y_predicted.append(rf_model.predict(X_test.iloc[i:i+test_batch_size].to_numpy()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = pd.DataFrame()\n",
    "ans['PRODUCT_ID'] = df_test['PRODUCT_ID']\n",
    "ans['PRODUCT_LENGTH'] = list(chain(*y_predicted))\n",
    "\n",
    "ans.to_csv('submission_sklearn.csv', index=False)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2e1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted_best = []\n",
    "\n",
    "# # batches of 10,000:\n",
    "# test_batch_size = 10000\n",
    "# for i in range(0, len(X_test) + test_batch_size, test_batch_size):\n",
    "#     y_predicted_best.append(rf_best.predict(X_test.iloc[i:i+test_batch_size].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e4044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans2 = pd.DataFrame()\n",
    "# ans2['PRODUCT_ID'] = df_test['PRODUCT_ID']\n",
    "# ans2['PRODUCT_LENGTH'] = list(chain(*y_predicted_best))\n",
    "\n",
    "# ans2.to_csv('submission_sklearn_best.csv', index=False)\n",
    "# ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f410736",
   "metadata": {},
   "source": [
    "## With Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e500eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/software/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/software/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/software/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|███████████████████████████████████| 99000/99000 [00:56<00:00, 1758.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "X_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "X_train_num = [row[:-2].astype(float) for row in X_train]\n",
    "X_train_text = [row[-1] for row in X_train]\n",
    "\n",
    "tokenized = []\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "for text in tqdm(X_train_text):\n",
    "    # Filter alphabet words only and non stop words, make it loser case\n",
    "    words = [word.lower() for word in word_tokenize(text) if ((word.isalpha()==1) & (word not in stop))]\n",
    "    # Lemmatize words \n",
    "    tokens = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "all_words = [word for text in tokenized for word in text]\n",
    "counts = Counter(all_words)\n",
    "bow = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "id2vocab = {v: k for k, v in vocab.items()}\n",
    "token_ids = [[vocab[word] for word in text_words] for text_words in tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6c057f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataloader\n",
    "def dataloader(text_inputs, num_inputs, labels, sequence_length=200, batch_size=16, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = list(range(len(text_inputs)))\n",
    "        random.shuffle(indices)\n",
    "        text_inputs = [text_inputs[idx] for idx in indices]\n",
    "        num_inputs = [num_inputs[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(text_inputs)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_texts = text_inputs[ii: ii+batch_size]        \n",
    "        batch = torch.zeros((sequence_length, len(batch_texts)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_texts):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_texts)])\n",
    "        num_tensor = torch.tensor(num_inputs[ii: ii+len(batch_texts)])\n",
    "        \n",
    "        yield batch, num_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9f13b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Model taking non-text and text inputs as well\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, dense_size, num_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size + num_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self, nn_input_text, nn_input_num, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on nn_input\n",
    "        \"\"\"\n",
    "        batch_size = nn_input_text.size(0)\n",
    "        nn_input_text = nn_input_text.long()\n",
    "        embeds = self.embedding(nn_input_text)\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        # Stack up LSTM outputs, apply dropout\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Dense layer\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        # Concatinate the dense output and meta inputs\n",
    "        concat_layer = torch.cat((dense_out, nn_input_num.float()), 1)\n",
    "        out = self.fc2(concat_layer)\n",
    "        logps = self.softmax(out)\n",
    "\n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "170ae062",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_input_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train_num[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "n_input_num = len(X_train_num[0])\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "548aea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 1/3 #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 12375/12375 [05:21<00:00, 38.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 2/3 #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 12375/12375 [05:23<00:00, 38.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 3/3 #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 12375/12375 [05:24<00:00, 38.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Training parameters\n",
    "epochs=3\n",
    "batch_size=8\n",
    "learning_rate=1e-4\n",
    "sequence_length=200\n",
    "clip=5\n",
    "\n",
    "# Set model\n",
    "model = TextClassifier(len(vocab)+1, 512, 128, 8, n_input_num, 3, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "# Set Loss Functoin and Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start Training\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(\"###### Epoch {}/{} #####\".format(epoch+1, epochs))\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    # Training Step\n",
    "    for text_batch, mum_batch, labels in tqdm(dataloader(token_ids, X_train_num, y_train, \n",
    "            batch_size=batch_size, sequence_length=sequence_length, shuffle=False),\n",
    "                                              total=int(len(y_train)/batch_size)):\n",
    "        # Skip the last batch of which size is not equal to batch_size\n",
    "        if text_batch.size(1) != batch_size:\n",
    "            break\n",
    "        # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        # Set Device\n",
    "        #text_batch, mum_batch, labels = np.float32(text_batch),np.float32(mum_batch),np.float32(labels)\n",
    "        text_batch, mum_batch, labels = text_batch.to(device), mum_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, mum_batch, hidden)\n",
    "        #loss = criterion(output, labels)\n",
    "        #loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1f65523",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'stoi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a positive review.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the input text using the Vocab object\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m [vocab\u001b[38;5;241m.\u001b[39mstoi(token, vocab\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m input_text\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Pad the input tokens to a fixed length\u001b[39;00m\n\u001b[1;32m      8\u001b[0m padded_input_tokens \u001b[38;5;241m=\u001b[39m pad_sequences([input_tokens], maxlen\u001b[38;5;241m=\u001b[39mn_input_num, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[103], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a positive review.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the input text using the Vocab object\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoi\u001b[49m(token, vocab\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m input_text\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Pad the input tokens to a fixed length\u001b[39;00m\n\u001b[1;32m      8\u001b[0m padded_input_tokens \u001b[38;5;241m=\u001b[39m pad_sequences([input_tokens], maxlen\u001b[38;5;241m=\u001b[39mn_input_num, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'stoi'"
     ]
    }
   ],
   "source": [
    "# Define the input text that you want to classify\n",
    "input_text = \"This is a positive review.\"\n",
    "\n",
    "# Tokenize the input text using the Vocab object\n",
    "input_tokens = [vocab.stoi(token, vocab.stoi['<unk>']) for token in input_text.split()]\n",
    "\n",
    "# Pad the input tokens to a fixed length\n",
    "padded_input_tokens = pad_sequences([input_tokens], maxlen=n_input_num, padding='post')\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(padded_input_tokens)\n",
    "\n",
    "# Print the predicted class\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec358946",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "convert_text_to_input_num() missing 1 required positional argument: 'sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict_numerical_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis a text of 10 times the power of the length of smae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 38\u001b[0m, in \u001b[0;36mpredict_numerical_values\u001b[0;34m(model, input_text)\u001b[0m\n\u001b[1;32m     36\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m tokenize_text(input_text)\n\u001b[1;32m     37\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m convert_tokens_to_ids(tokenized_text, vocab)\n\u001b[0;32m---> 38\u001b[0m input_num \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_text_to_input_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmum_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n\u001b[1;32m     41\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: convert_text_to_input_num() missing 1 required positional argument: 'sequence_length'"
     ]
    }
   ],
   "source": [
    "predict_numerical_values(model, \"This a text of 10 times the power of the length of smae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8193f9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize mum_batch and hidden tensors\n",
    "batch_size = 1\n",
    "mum_batch = torch.zeros(batch_size, sequence_length).long()\n",
    "hidden = tuple(h.to(device) for h in model.init_hidden(batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f9eb252",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "LSTM: Expected input to be 2-D or 3-D but received 4-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Pass preprocessed data through model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmum_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert output to labels\u001b[39;00m\n\u001b[1;32m     14\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[62], line 42\u001b[0m, in \u001b[0;36mTextClassifier.forward\u001b[0;34m(self, nn_input_text, nn_input_num, hidden_state)\u001b[0m\n\u001b[1;32m     40\u001b[0m nn_input_text \u001b[38;5;241m=\u001b[39m nn_input_text\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     41\u001b[0m embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(nn_input_text)\n\u001b[0;32m---> 42\u001b[0m lstm_out, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Stack up LSTM outputs, apply dropout\u001b[39;00m\n\u001b[1;32m     44\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m lstm_out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:795\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    793\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    797\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: LSTM: Expected input to be 2-D or 3-D but received 4-D tensor"
     ]
    }
   ],
   "source": [
    "input_string = \"This is an example input string.\"\n",
    "preprocessed_data = preprocess_input(input_string, sequence_length, vocab)\n",
    "\n",
    "# Initialize mum_batch and hidden tensors\n",
    "batch_size = 1\n",
    "mum_batch = torch.zeros(batch_size, sequence_length).long()\n",
    "hhidden = tuple(h.to(device) for h in model.init_hidden(batch_size))# add to(device) to ensure that the tensor is on the correct device\n",
    "\n",
    "# Pass preprocessed data through model\n",
    "with torch.no_grad():\n",
    "    output, hidden = model(preprocessed_data.unsqueeze(0), mum_batch, hidden)\n",
    "\n",
    "# Convert output to labels\n",
    "predicted_labels = torch.argmax(output, dim=1)\n",
    "\n",
    "print(\"Predicted label:\", predicted_labels.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b9ccc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(105362, 512)\n",
       "  (lstm): LSTM(512, 128, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2aceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame()\n",
    "X_test['PRODUCT_TYPE_ID'] = df_test['PRODUCT_TYPE_ID']\n",
    "X_test['TEXT'] =  df_test['TITLE'] + ' ' + df_test['DESCRIPTION'] + ' ' + df_test['BULLET_POINTS']\n",
    "\n",
    "\n",
    "test =X_test['TEXT'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512cf3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predict(model,test,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a66ecfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame()\n",
    "X_test['PRODUCT_TYPE_ID'] = df_test['PRODUCT_TYPE_ID']\n",
    "X_test['TEXT'] =  df_test['TITLE'] + ' ' + df_test['DESCRIPTION'] + ' ' + df_test['BULLET_POINTS']\n",
    "\n",
    "\n",
    "y_predicted = []\n",
    "\n",
    "# batches of 10,000:\n",
    "test_batch_size = 10000\n",
    "for i in range(0, len(X_test) , test_batch_size):\n",
    "    try:\n",
    "        value = torch.tensor(X_test.iloc[i:i+test_batch_size].to_numpy())\n",
    "        y_predicted.append(model(value))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "y_predicted\n",
    "\n",
    "#ans1 = pd.DataFrame()\n",
    "#ans1['PRODUCT_ID'] = df_test['PRODUCT_ID']\n",
    "#ans1['PRODUCT_LENGTH'] = list(chain(*y_predicted))\n",
    "\n",
    "#ans1.to_csv('submission_sklearn.csv', index=False)\n",
    "#ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b9b23e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Model definition\n",
    "class BertTextClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, dense_size, meta_size, output_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size \n",
    "        self.dropout = dropout        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.weights = nn.Parameter(torch.rand(13, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size + meta_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, nn_input_meta):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on nn_input\n",
    "        \"\"\"\n",
    "        all_hidden_states, all_attentions = self.bert(input_ids)[-2:]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n",
    "        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n",
    "        atten = F.softmax(atten.view(-1), dim=0)\n",
    "        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])        \n",
    "        # Dense layer\n",
    "        dense_out = self.fc1(self.dropout(feature))\n",
    "        # Concatinate the dense output and meta inputs\n",
    "        concat_layer = torch.cat((dense_out, nn_input_meta.float()), 1)\n",
    "        out = self.fc2(concat_layer)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7148ec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.modules.module.Module.eval(self: ~T) -> ~T>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f786d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, meta):\n",
    "    \"\"\"\n",
    "    Predict the class for the given input text and meta features\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Perform a forward pass on the model\n",
    "    output = model(inputs['input_ids'], meta)\n",
    "    # Get the predicted class index\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    return predicted.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "07171905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_weights.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the model and load the trained weights\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BertTextClassifier(hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, dense_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, meta_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_weights.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the input text and meta features\u001b[39;00m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis movie is terrible!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:793\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    791\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:273\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:254\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_weights.pth'"
     ]
    }
   ],
   "source": [
    "# Initialize the model and load the trained weights\n",
    "model = BertTextClassifier(hidden_size=768, dense_size=256, meta_size=10, output_size=2, dropout=0.1)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# Define the input text and meta features\n",
    "text = \"This movie is terrible!\"\n",
    "meta = torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float32)\n",
    "\n",
    "# Predict the class for the input text\n",
    "predicted_class = predict(model, text, meta)\n",
    "\n",
    "# Print the predicted class\n",
    "if predicted_class == 0:\n",
    "    print(\"The model predicts that the input text is negative.\")\n",
    "else:\n",
    "    print(\"The model predicts that the input text is positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246991f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
