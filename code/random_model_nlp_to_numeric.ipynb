{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8402d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv').fillna('')\n",
    "df_test = pd.read_csv('dataset/test.csv').fillna('')\n",
    "# df_sample = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef8c888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>BULLET_POINTS</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRODUCT_TYPE_ID</th>\n",
       "      <th>PRODUCT_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1925202</td>\n",
       "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
       "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
       "      <td></td>\n",
       "      <td>1650</td>\n",
       "      <td>2125.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2673191</td>\n",
       "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
       "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
       "      <td></td>\n",
       "      <td>2755</td>\n",
       "      <td>393.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2765088</td>\n",
       "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
       "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
       "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
       "      <td>7537</td>\n",
       "      <td>748.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1594019</td>\n",
       "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
       "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
       "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
       "      <td>2996</td>\n",
       "      <td>787.401574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283658</td>\n",
       "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>6112</td>\n",
       "      <td>598.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249693</th>\n",
       "      <td>2422167</td>\n",
       "      <td>Nike Women's As W Ny Df Swsh Hn Kh Bra (CZ7610...</td>\n",
       "      <td>Material : Polyester</td>\n",
       "      <td></td>\n",
       "      <td>3009</td>\n",
       "      <td>1181.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249694</th>\n",
       "      <td>2766635</td>\n",
       "      <td>(3PCS) Goose Game Cute Cartoon Enamel Pins, Fu...</td>\n",
       "      <td>[❤ [Inspiration] Inspired by the Untitled Goos...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;[Brand]: &lt;/b&gt;XVIEONR&lt;/p&gt; &lt;p&gt;&lt;br&gt;&lt;/p&gt; &lt;p&gt;...</td>\n",
       "      <td>3413</td>\n",
       "      <td>125.984252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249695</th>\n",
       "      <td>1987786</td>\n",
       "      <td>Kangroo Sweep Movement Printed Wooden Wall Clo...</td>\n",
       "      <td>[Dial size: 12 inches in diameter,Big, clear r...</td>\n",
       "      <td>Wall Clocks Are Very Attractive In Looks And E...</td>\n",
       "      <td>1574</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249696</th>\n",
       "      <td>1165754</td>\n",
       "      <td>Electro Voice EKX-BRKT15 | Wall Mount Bracket ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>592</td>\n",
       "      <td>2900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249697</th>\n",
       "      <td>1072666</td>\n",
       "      <td>Skyjacker C7360SP Component Box For PN[C7360PK...</td>\n",
       "      <td>[Component Box For PN[C7360PK],4 in. Lift,Incl...</td>\n",
       "      <td>Skyjacker C7360SP Component Box For PN[C7360PK...</td>\n",
       "      <td>7367</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249698 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRODUCT_ID                                              TITLE  \\\n",
       "0           1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
       "1           2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
       "2           2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
       "3           1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
       "4            283658  The United Empire Loyalists: A Chronicle of th...   \n",
       "...             ...                                                ...   \n",
       "2249693     2422167  Nike Women's As W Ny Df Swsh Hn Kh Bra (CZ7610...   \n",
       "2249694     2766635  (3PCS) Goose Game Cute Cartoon Enamel Pins, Fu...   \n",
       "2249695     1987786  Kangroo Sweep Movement Printed Wooden Wall Clo...   \n",
       "2249696     1165754  Electro Voice EKX-BRKT15 | Wall Mount Bracket ...   \n",
       "2249697     1072666  Skyjacker C7360SP Component Box For PN[C7360PK...   \n",
       "\n",
       "                                             BULLET_POINTS  \\\n",
       "0        [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
       "1        [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
       "2        [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
       "3        [Made By 95%cotton and 5% Lycra which gives yo...   \n",
       "4                                                            \n",
       "...                                                    ...   \n",
       "2249693                               Material : Polyester   \n",
       "2249694  [❤ [Inspiration] Inspired by the Untitled Goos...   \n",
       "2249695  [Dial size: 12 inches in diameter,Big, clear r...   \n",
       "2249696                                                      \n",
       "2249697  [Component Box For PN[C7360PK],4 in. Lift,Incl...   \n",
       "\n",
       "                                               DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
       "0                                                                      1650   \n",
       "1                                                                      2755   \n",
       "2        Specifications: Color: Red, Material: Aluminiu...             7537   \n",
       "3        AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
       "4                                                                      6112   \n",
       "...                                                    ...              ...   \n",
       "2249693                                                                3009   \n",
       "2249694  <p><b>[Brand]: </b>XVIEONR</p> <p><br></p> <p>...             3413   \n",
       "2249695  Wall Clocks Are Very Attractive In Looks And E...             1574   \n",
       "2249696                                                                 592   \n",
       "2249697  Skyjacker C7360SP Component Box For PN[C7360PK...             7367   \n",
       "\n",
       "         PRODUCT_LENGTH  \n",
       "0           2125.980000  \n",
       "1            393.700000  \n",
       "2            748.031495  \n",
       "3            787.401574  \n",
       "4            598.424000  \n",
       "...                 ...  \n",
       "2249693     1181.100000  \n",
       "2249694      125.984252  \n",
       "2249695     1200.000000  \n",
       "2249696     2900.000000  \n",
       "2249697     2000.000000  \n",
       "\n",
       "[2249698 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5321f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['PRODUCT_LENGTH'].astype(int)\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['PRODUCT_TYPE_ID'] = df_train['PRODUCT_TYPE_ID']\n",
    "X['TEXT'] =  df_train['TITLE'][:1000] + ' ' + df_train['DESCRIPTION'][:1000] + ' ' + df_train['BULLET_POINTS'][:1000]\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389abbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_data(x):\n",
    "    return [record[:-2].astype(float) for record in x]\n",
    "\n",
    "def get_text_data(x):\n",
    "    return [record[-1] for record in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af4b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 20 candidates, totalling 140 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 140 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n140 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1192, in fit_transform\n    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1214, in _parallel_func\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 445, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2131, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1274, in _count_vocab\n    for feature in analyze(doc):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 111, in _analyze\n    doc = preprocessor(doc)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n    doc = doc.lower()\nAttributeError: 'float' object has no attribute 'lower'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Perform GridSearch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mkfold, scoring\u001b[38;5;241m=\u001b[39mscoring, \n\u001b[1;32m     37\u001b[0m                          refit\u001b[38;5;241m=\u001b[39mrefit, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m rf_best \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 140 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n140 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1192, in fit_transform\n    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1214, in _parallel_func\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 445, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2131, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1274, in _count_vocab\n    for feature in analyze(doc):\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 111, in _analyze\n    doc = preprocessor(doc)\n  File \"/Users/software/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n    doc = doc.lower()\nAttributeError: 'float' object has no attribute 'lower'\n"
     ]
    }
   ],
   "source": [
    "transfomer_numeric = FunctionTransformer(get_numeric_data)\n",
    "transformer_text = FunctionTransformer(get_text_data)\n",
    "\n",
    "# Create a pipeline to concatenate Tfidf Vector and Numeric data\n",
    "# Use RandomForestClassifier as an example\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', transfomer_numeric)\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', transformer_text),\n",
    "                ('vec', TfidfVectorizer(analyzer='word'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Grid Search Parameters for RandomForest\n",
    "param_grid = {'clf__n_estimators': np.linspace(1, 100, 10, dtype=int),\n",
    "              'clf__min_samples_split': [3, 10],\n",
    "              'clf__min_samples_leaf': [3],\n",
    "              'clf__max_features': [7],\n",
    "              'clf__max_depth': [None],\n",
    "              'clf__criterion': ['gini'],\n",
    "              'clf__bootstrap': [False]}\n",
    "\n",
    "# Training config\n",
    "kfold = KFold(n_splits=7)# StratifiedKFold(n_splits=7)\n",
    "scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
    "refit = 'F1'\n",
    "\n",
    "X_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform GridSearch\n",
    "rf_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold, scoring=scoring, \n",
    "                         refit=refit, n_jobs=-1, return_train_score=True, verbose=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_best = rf_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac956116",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame()\n",
    "X_test['PRODUCT_TYPE_ID'] = df_test['PRODUCT_TYPE_ID']\n",
    "X_test['TEXT'] =  df_test['TITLE'] + ' ' + df_test['DESCRIPTION'] + ' ' + df_test['BULLET_POINTS']\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "y_predicted = rf_model.predict(X_test)\n",
    "\n",
    "ans = pd.DataFrame()\n",
    "ans['PRODUCT_ID'] = df_test['PRODUCT_ID']\n",
    "ans['PRODUCT_LENGTH'] = y_predicted\n",
    "\n",
    "ans.to_csv('submission_sklearn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca00e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "X_train_num = [row[:-2].astype(float) for row in X_train]\n",
    "X_train_text = [row[-1] for row in X_train]\n",
    "\n",
    "tokenized = []\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "for text in tqdm(X_train_text):\n",
    "    # Filter alphabet words only and non stop words, make it loser case\n",
    "    words = [word.lower() for word in word_tokenize(text) if ((word.isalpha()==1) & (word not in stop))]\n",
    "    # Lemmatize words \n",
    "    tokens = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "all_words = [word for text in tokenized for word in text]\n",
    "counts = Counter(all_words)\n",
    "bow = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "id2vocab = {v: k for k, v in vocab.items()}\n",
    "token_ids = [[vocab[word] for word in text_words] for text_words in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataloader\n",
    "def dataloader(text_inputs, num_inputs, labels, sequence_length=200, batch_size=16, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = list(range(len(text_inputs)))\n",
    "        random.shuffle(indices)\n",
    "        text_inputs = [text_inputs[idx] for idx in indices]\n",
    "        num_inputs = [num_inputs[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(text_inputs)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_texts = text_inputs[ii: ii+batch_size]        \n",
    "        batch = torch.zeros((sequence_length, len(batch_texts)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_texts):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_texts)])\n",
    "        num_tensor = torch.tensor(num_inputs[ii: ii+len(batch_texts)])\n",
    "        \n",
    "        yield batch, num_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb262b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Model taking non-text and text inputs as well\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, dense_size, num_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size + num_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self, nn_input_text, nn_input_num, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on nn_input\n",
    "        \"\"\"\n",
    "        batch_size = nn_input_text.size(0)\n",
    "        nn_input_text = nn_input_text.long()\n",
    "        embeds = self.embedding(nn_input_text)\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        # Stack up LSTM outputs, apply dropout\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Dense layer\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        # Concatinate the dense output and meta inputs\n",
    "        concat_layer = torch.cat((dense_out, nn_input_num.float()), 1)\n",
    "        out = self.fc2(concat_layer)\n",
    "        logps = self.softmax(out)\n",
    "\n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_num = len(X_train_num[0])\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10327ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Training parameters\n",
    "epochs=3\n",
    "batch_size=8\n",
    "learning_rate=1e-4\n",
    "sequence_length=200\n",
    "clip=5\n",
    "\n",
    "# Set model\n",
    "model = TextClassifier(len(vocab)+1, 512, 128, 8, n_input_num, 3, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "# Set device\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "# Set Loss Functoin and Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start Training\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(\"###### Epoch {}/{} #####\".format(epoch+1, epochs))\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    # Training Step\n",
    "    for text_batch, mum_batch, labels in tqdm(dataloader(token_ids, X_train_num, y_train, \n",
    "            batch_size=batch_size, sequence_length=sequence_length, shuffle=False),\n",
    "                                              total=int(len(y_train)/batch_size, desc=\"Training Batch\")):\n",
    "        # Skip the last batch of which size is not equal to batch_size\n",
    "        if text_batch.size(1) != batch_size:\n",
    "            break\n",
    "        # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        # Set Device\n",
    "        text_batch, mum_batch, labels = text_batch.to(device), mum_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, mum_batch, hidden)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation Step\n",
    "    # :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c916b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Model definition\n",
    "class BertTextClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, dense_size, meta_size, output_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.weights = nn.Parameter(torch.rand(13, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size + meta_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, nn_input_meta):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on nn_input\n",
    "        \"\"\"\n",
    "        all_hidden_states, all_attentions = self.bert(input_ids)[-2:]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n",
    "        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n",
    "        atten = F.softmax(atten.view(-1), dim=0)\n",
    "        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])        \n",
    "        # Dense layer\n",
    "        dense_out = self.fc1(self.dropout(feature))\n",
    "        # Concatinate the dense output and meta inputs\n",
    "        concat_layer = torch.cat((dense_out, nn_input_meta.float()), 1)\n",
    "        out = self.fc2(concat_layer)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b724f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
